
## 实验报告：全流程创新与改进（术语深度解析版）

本实验在物体识别的基础任务（CIFAR-10 分类）上，对实验手册中提供的基准流程（基于 MindSpore 框架的 LeNet-5 网络）进行了全面的、面向 SOTA（State-of-the-Art，当前最佳水平）技术的创新与改进。

### 一、核心模型架构的跨越式升级与对比分析

原实验手册采用的是经典的 **LeNet-5 卷积神经网络（CNN）**架构，属于浅层 CNN。本实验则采用了深度学习领域最前沿的 **Vision Transformer** 家族代表——**Swin Transformer**，实现了架构的根本性突破。

#### 1. 架构图对比（文字描述）

| 特征 | 基准模型：LeNet-5 (CNN) | 创新模型：Swin Transformer (ViT) |
| :--- | :--- | :--- |
| **基础单元** | 卷积层（Convolution）、池化层（Pooling） | **Window Attention**（窗口自注意力） |
| **整体架构** | **平面架构：** 卷积层堆叠，特征图分辨率逐步降低，通道数增加。 | **分层架构：** 类似于 CNN，通过 Patch Merging（特征融合）实现特征图分辨率减半，通道数加倍。 |
| **特征提取** | 局部特征提取；感受野通过多层卷积堆叠增大。 | **全局/局部混合提取：** 窗口内局部自注意力；窗口间信息交互（Shifted Window），实现了全局感知能力。 |

#### 2. Swin Transformer 关键术语解析

| 术语 | 详细解释 |
| :--- | :--- |
| **Vision Transformer (ViT)** | **视觉Transformer：** 一种基于 Google Transformer 架构（最初用于自然语言处理）的图像处理模型。它将图像切分成一系列 Patch（图像块），并将这些 Patch 视为序列中的“词元”（Token）输入到 Transformer 编码器中进行处理。 |
| **Swin Transformer** | **Swin Transformer：** ViT 的一种高效变体。它解决了原始 ViT 复杂度高的问题，引入了**层次化结构**和**移动窗口机制**。 |
| **层次化设计** | **Hierarchy：** 指模型通过 **Patch Merging** 模块，像 CNN 一样分阶段处理特征。每经过一个阶段，特征图的分辨率降低（如 $32 \times 32 \to 16 \times 16$），而通道数增加（如 $96 \to 192$），使其具备处理多尺度物体的能力。 |
| **局部窗口自注意力** | **Window Attention：** 在 Swin Transformer 中，自注意力计算被限制在预先划分的**不重叠的小窗口**内（本实验为 $4 \times 4$），而非在整张图片上计算。这大幅降低了计算复杂度（从二次方 $O(H^2W^2)$ 降为线性 $O(HW)$）。 |
| **移动窗口机制** | **Shifted Window：** 通过在相邻层之间进行**窗口的循环位移**，使窗口的边界发生变化。这确保了位于原窗口边界的 Patch 能够在下一层有机会与不同窗口内的 Patch 交互，实现了局部特征的全局信息流通。 |

### 二、数据预处理与高级数据增强（SOTA 正则化）

本实验引入了三大 SOTA 级别的正则化和数据增强技术，旨在克服大型 Transformer 模型对数据量的饥渴并防止过拟合，从而提高模型的鲁棒性和泛化能力。

| 术语 | 详细解释 |
| :--- | :--- |
| **AutoAugment** | **自动增强：** 一种通过搜索算法自动发现适用于特定数据集（如 CIFAR-10）的最佳数据增强策略集合的方法。它避免了人为设计的局限性，通常能带来显著的性能提升。 |
| **RandomErasing** | **随机擦除：** 一种正则化方法，以一定概率（p=0.25）随机选择图像中的一个矩形区域，并将其像素值擦除（如设为 0 或随机噪声）。**效果：** 强制模型学习更稳健、更全面的特征表示，减少对局部关键区域的依赖。 |
| **MixUp** | **混合训练：** 一种数据增强策略，通过对两个随机样本的图像 $\mathbf{x}_i, \mathbf{x}_j$ 及其对应的 One-Hot 标签 $y_i, y_j$ 进行**线性插值**来生成新的训练样本 $(\tilde{\mathbf{x}}, \tilde{\mathbf{y}})$。 **公式：** $\tilde{\mathbf{x}} = \lambda \mathbf{x}_i + (1-\lambda) \mathbf{x}_j$；$\tilde{\mathbf{y}} = \lambda \mathbf{y}_i + (1-\lambda) \mathbf{y}_j$。 |
| **CutMix** | **裁剪混合：** 另一种数据增强策略，将一张图像的矩形区域剪切，并粘贴到另一张图像上。新样本的标签是按**区域面积比例**混合得到的。 **效果：** 训练效率通常高于 MixUp，同时保持了平滑决策边界的优势。 |
| **泛化能力 (Generalization)** | **泛化能力：** 指训练好的模型在未见过的数据集（测试集）上保持高性能的能力。增强数据是提高泛化能力的主要手段。 |

### 三、训练优化策略与收敛稳定性

本实验采用了为训练高性能 Vision Transformer 模型设计的先进优化策略，以确保模型稳定收敛并达到最优性能。

| 术语 | 详细解释 |
| :--- | :--- |
| **AdamW 优化器** | **AdamW 优化器：** 在经典的 Adam 优化器的基础上，解决了 Adam 在执行 **L2 正则化 (L2 Regularization)** 时处理权重衰减（Weight Decay）不正确的问题。 **优势：** 对于带有 L2 正则化的模型（如 Swin-T），AdamW 能提供更优的性能和更好的泛化能力。 |
| **权重衰减 (Weight Decay)** | **权重衰减：** 相当于 L2 正则化，通过在损失函数中添加模型权重的平方项，惩罚大的权重值。**作用：** 限制模型的复杂度，防止过拟合。 |
| **学习率 Warmup** | **学习率热身：** 在训练的最初几个 Epoch（本实验为 5 轮），将学习率从一个非常小的值（如 0）逐渐线性增加到预设的基准学习率（Base LR）。**作用：** 稳定大型模型（如 Transformer）在训练初期的梯度，防止因初始学习率过高导致的训练震荡或梯度爆炸。 |
| **余弦退火** | **Cosine Annealing：** 一种学习率调度策略，学习率随 Epoch 变化呈**余弦函数**下降趋势。学习率先缓慢下降，加速下降，然后再次缓慢下降至最小值。**优势：** 相比阶梯式下降，它使学习率变化更平滑，有助于模型在训练后期精细搜索最优解。 |
| **标签平滑** | **Label Smoothing：** 一种正则化技巧，将 One-Hot 标签（硬标签）转换为略微平滑的标签（软标签）。 **公式：** $y'_{k} = (1-\epsilon)y_{k} + \epsilon/K$，其中 $\epsilon$ 为平滑因子 (0.1)，$K$ 为类别数。 **作用：** 减少模型对错误标签的过度拟合，提高模型对不确定性的建模能力。 |
| **梯度裁剪** | **Gradient Clipping：** 一种训练稳定性技术。在反向传播计算梯度后，如果梯度的**范数**超过预设的阈值（`max_norm`），则对其进行等比例缩小。 **作用：** 避免在训练大型或深层网络时出现梯度爆炸（Gradient Explosion），确保训练过程稳定。 |